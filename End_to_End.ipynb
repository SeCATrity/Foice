{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b610c41-3af7-4925-8561-7fd4ceecb872",
   "metadata": {},
   "source": [
    "# Demonstration of Voice Synthesis using Only a Single Face Photo\n",
    "\n",
    "### Please first setup the required environment to include the following libraries\n",
    "* face-alignment (pip install face-alignment)\n",
    "* numpy\n",
    "* cv2\n",
    "* torch\n",
    "* torchvision\n",
    "* matplotlib\n",
    "\n",
    "### Please go through each of the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01d28d",
   "metadata": {},
   "source": [
    "#### Step 1: Put all test images in the folder \"../Test_Images/\". Name each person's face photo as \"{NAME}.jpg\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e8c74",
   "metadata": {},
   "source": [
    "#### Step 2: Modify the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69643ce5-aa52-40ff-9b72-d7e607643f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = \"Alice\" # Make sure the person's name matches the name of the image. For example, person = \"Alice\" for ../Test_Images/Alice.jpg\n",
    "num = 100 # The numnber of audio output files to generate\n",
    "text = 'One two three four five six seven eight.' # The content of the audio to be generated. Make sure the numnber matches the digits displayed on the screen (e.g., 12345678)\n",
    "latent_dim = 48 # we use 48 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028f986",
   "metadata": {},
   "source": [
    "#### Step 3: Run the following cell. The audio outputs will be in the output folder \"../Test_Outputs/\", following the name of the person (e.g., Alice_1.wav)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3cb2d-d52c-4780-aae6-1b2ec0044d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_alignment\n",
    "import cv2\n",
    "import numpy as np\n",
    "from synthesize import *\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from cvae_model import CVAE\n",
    "import torch\n",
    "\n",
    "global fa        \n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, flip_input=False, device='cuda')\n",
    "\n",
    "def align(image, desiredLeftEye=(0.3, 0.3), desiredFaceWidth=256):\n",
    "\t# convert the landmark (x, y)-coordinates to a NumPy array\n",
    "\t########################## Detect 68 landmarks ##########################\n",
    "\tlandmarks = fa.get_landmarks(image) \n",
    "\t# If there's no face detected or more than one face detected, then return the original image\n",
    "\tif landmarks is None or len(landmarks) != 1:\n",
    "\t\tprint('Not Aligned')\n",
    "\t\treturn None\n",
    "\n",
    "\tlandmark = landmarks[0]\n",
    "\n",
    "\tright_eye_points = list(range(36,42))\n",
    "\tleft_eye_points = list(range(42,48))\n",
    "\n",
    "\t# extract the left and right eye (x, y)-coordinates\n",
    "\n",
    "\tlandmarks_left, landmarks_right = landmark[left_eye_points], landmark[right_eye_points]\n",
    "\n",
    "\tlStart, lEnd = np.array([int(x[0]) for x in landmarks_left]), np.array([int(x[1]) for x in landmarks_left])\n",
    "\trStart, rEnd = np.array([int(x[0]) for x in landmarks_right]), np.array([int(x[1]) for x in landmarks_right])\n",
    "\n",
    "\t# compute the center of mass for each eye\n",
    "\tleftEyeCenter = (lStart.mean().astype(\"int\"), lEnd.mean().astype(\"int\"))\n",
    "\trightEyeCenter = (rStart.mean().astype(\"int\"), rEnd.mean().astype(\"int\"))\n",
    "\n",
    "\t# compute the angle between the eye centroids\n",
    "\tdY = rightEyeCenter[1] - leftEyeCenter[1]\n",
    "\tdX = rightEyeCenter[0] - leftEyeCenter[0]\n",
    "\tangle = np.degrees(np.arctan2(dY, dX)) - 180\n",
    "\n",
    "\t# compute the desired right eye x-coordinate based on the\n",
    "\t# desired x-coordinate of the left eye\n",
    "\tdesiredRightEyeX = 1.0 - desiredLeftEye[0]\n",
    "\n",
    "\t# determine the scale of the new resulting image by taking\n",
    "\t# the ratio of the distance between eyes in the *current*\n",
    "\t# image to the ratio of distance between eyes in the\n",
    "\t# *desired* image\n",
    "\tdist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "\tdesiredDist = (desiredRightEyeX - desiredLeftEye[0])\n",
    "\tdesiredDist *= desiredFaceWidth\n",
    "\tscale = desiredDist / dist\n",
    "\n",
    "\t# compute center (x, y)-coordinates (i.e., the median point)\n",
    "\t# between the two eyes in the input image\n",
    "\teyesCenter = (int((leftEyeCenter[0] + rightEyeCenter[0]) // 2),\n",
    "\t\tint((leftEyeCenter[1] + rightEyeCenter[1]) // 2))\n",
    "\n",
    "\t# grab the rotation matrix for rotating and scaling the face\n",
    "\tM = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
    "\n",
    "\t# update the translation component of the matrix\n",
    "\ttX = desiredFaceWidth * 0.5\n",
    "\ttY = desiredFaceWidth * desiredLeftEye[1]\n",
    "\tM[0, 2] += (tX - eyesCenter[0])\n",
    "\tM[1, 2] += (tY - eyesCenter[1])\n",
    "\n",
    "\t# apply the affine transformation\n",
    "\t(w, h) = (desiredFaceWidth, desiredFaceWidth)\n",
    "\toutput = cv2.warpAffine(image, M, (w, h),\n",
    "\tflags=cv2.INTER_CUBIC)\n",
    "\n",
    "\t# return the aligned face\n",
    "\treturn output\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_id = torch.cuda.current_device()\n",
    "    gpu_properties = torch.cuda.get_device_properties(device_id)\n",
    "    ## Print some environment information (for debugging purposes)\n",
    "    print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n",
    "        \"%.1fGb total memory.\\n\" %\n",
    "        (torch.cuda.device_count(),\n",
    "        device_id,\n",
    "        gpu_properties.name,\n",
    "        gpu_properties.major,\n",
    "        gpu_properties.minor,\n",
    "        gpu_properties.total_memory / 1e9))\n",
    "else:\n",
    "    print(\"Using CPU for inference.\\n\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CVAE(latent_size=latent_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"./F2V_models/CVAE_48.pth\"))\n",
    "model.eval()\n",
    "\n",
    "encoder.load_model(Path('./F2V_models/encoder.pt'))\n",
    "synthesizer = Synthesizer(Path('./F2V_models/synthesizer.pt'))\n",
    "vocoder.load_model(Path('./F2V_models/vocoder.pt'))\n",
    "\n",
    "input_face = cv2.imread(\"Test_Images/{}.jpg\".format(person))\n",
    "aligned_face = align(input_face)\n",
    "plt.imshow(aligned_face[...,::-1])\n",
    "\n",
    "transform_fn = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # transforms.RandomResizedCrop(224),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                        std  = [ 0.229, 0.224, 0.225 ]),\n",
    "    ])\n",
    "\n",
    "image = transform_fn(aligned_face)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "for i in range(num):\n",
    "    sampled_z = torch.tensor(np.random.normal(0, 1, (image.size(0), latent_dim))).to(device)\n",
    "    cvae_B, cnn_B = model.decode(sampled_z.float(), image.to(device))\n",
    "    cvae_B = cvae_B.squeeze(1)\n",
    "    cvae_B = cvae_B.detach().cpu().numpy()\n",
    "    cnn_B = cnn_B.detach().cpu().numpy()\n",
    "    cvae_B = cvae_B[0]\n",
    "    cnn_B = cnn_B[0]\n",
    "    embed = cvae_B\n",
    "    texts = [text]\n",
    "    embeds = [embed]\n",
    "    generated_wav = synthesize_audio(synthesizer, vocoder, texts, embeds)\n",
    "    filename = \"./Test_Outputs/{}_{}.wav\".format(person, i)\n",
    "    sf.write(filename, generated_wav.astype(np.float32), synthesizer.sample_rate)\n",
    "\n",
    "# Save CNN\n",
    "embed = cnn_B\n",
    "texts = [text]\n",
    "embeds = [embed]\n",
    "generated_wav = synthesize_audio(synthesizer, vocoder, texts, embeds)\n",
    "filename = \"./Test_Outputs/{}_CNN.wav\".format(person)\n",
    "sf.write(filename, generated_wav.astype(np.float32), synthesizer.sample_rate)\n",
    "\n",
    "print(\"{} Completed\".format(person))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e194e",
   "metadata": {},
   "source": [
    "### Step 4: Now use a digital speaker to play each of the generated audio file one by one in attempt to bypass the Voiceprint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
